---
title: "ゼロからつくるDeep Learningを読んだ"
date: "2022/03/07"
---

# はじめに

どうも、o-xian です。  
先日、4 年次の研究室がほぼ決まりました。研究テーマとして機械学習・深層学習を扱う予定です。  
いままで深層学習を使った画像認識は触ったことがあるのですが、Keras など使って「雰囲気で」やっている節がありました。そのため、「なんか知らんけど学習できた」という状態に陥っていました。研究テーマの分野でこの状態はさすがによろしくないでしょう。ということで、[ゼロからつくる Deep Learning](https://www.oreilly.co.jp/books/9784873117584/)(以下、ゼロから本)という書籍でディープラーニングを基礎から入門しなおすことにしました。  
今回は、ゼロから本の中でも、特に印象に残ったこと・感想などを混ぜながらまとめていきます。

# 1 章 Python 入門

第 1 章では Python の基本的な文法のほか、深層学習を行う際によく使うライブラリなどを学びました。Python はほぼ初めての状態でしたが、理解にそこまで時間はかからなかったです。個人的には以下の点が特に印象に残りました。

### NumPy と行列計算

ディープラーニングを実装する際は、配列や行列の計算を多く行います。この計算を簡単に行ってくれるのが NumPy です。例えば 2 × 2 の行列計算の場合、NumPy で次のように計算できます。

```py
import numpy as np

A = np.array([1, 2], [3, 4])
print(A.shape)
# (2, 2)

B = np.array([5, 6], [7, 8])
print(B.shape)
# (2, 2)

np.dot(A, B)
# array([[19, 22],
#        [43, 50]])

```

ちなみに、Python 以外でプログラミング言語の機能として行列計算ができるのは、Fortran(90 以降)、R、Julia、C++くらいだそうです。ゼロから本を読み進めていくと分かるのですが、ディープラーニングでは膨大な量の行列計算を行います。そのため、言語レベルで行列計算をサポートしている言語がディープラーニングに適しているのだろうと思いました。

### Matprotlib

Matprotlib はグラフ描画やデータの可視化のためのライブラリです。グラフを描画する Python コードを次に示します。

```py
import numpy as np
import matplotlib.pyplot as plt

x = np.arange(0, 6, 0.1)
y = np.sin(x)

plt.plot(x, y)
plt.show
```

これを実行すると、次のようにグラフが描画されます。ゼロから本においては、ニューラルネットワークの活性化関数や学習の精度を論じる際にグラフを使っていました。

![](/blog/deep-learning-from-scratch/graph-matplotlib.png)

なぜこれが印象に残ったかというと、Jupyter Lab 上で Open CV の代わりに使ったことがあるからです。
ここでは詳しいことは書きませんが、Jupyter Lab 上では Open CV で処理した画像をそのまま`imshow`で表示するということができません。そのため、若干ハックじみた方法で画像を表示させます。その方法というのが、「`%matplotlib inline`を書いたうえで、`imshow`の代わりに Matplotlib の`show`を使う」というものです。この問題に結構な時間を溶かした経験があったので、私にとって Matpltlib は Python で画像がうまく扱えないときの応急処理要因というイメージでした。その Matplotlib が最初の章に出てきたので、肩透かしを食らった気分でした。  
今後は積極的に使っていくと思います。

# 2 章 パーセプトロン

ディープラーニングはニューラルネットワークの拡張であり、そのニューラルネットワークの最も基本的な単位がパーセプトロンです。パーセプトロンには、次の特徴があります。

- 入力の総和がある値(閾値)を越えると、 1 を出力する
- 各入力に重みという数をかけることで、入力の重要度をコントロールできる
- 入力の総和にバイアスという数を足すことで、1 が発火する度合いをコントロールできる

図に示したものがこちらになります([５分でわかる！パーセプトロンの仕組みと実装方法（Python） | あぱーブログ](https://blog.apar.jp/deep-learning/11979/)から拝借しました)。

![](https://cdn.apar.jp/wp-content/uploads/2019/05/ogp-perceptron.png)

このパーセプトロンは、複数重ねることで表現力を広げることができます。この拡張性がこの章ではかなり重要だったのではと思います。次章のニューラルネットワークからは、パーセプトロンをさらに拡張していきます。

# 3 章 ニューラルネットワーク

**※ここから先の内容は本書の直接的な内容により関わってくるため、詳しい言及は避けたいと思います。私の完全な感想記事になることをご了承ください。**

ニューラルネットワークは、パーセプトロンを何層にも重なり、信号を送り合います。また、前章のパーセプトロンからの繋がりとして、活性化関数を学びました。さらに活性化関数にはいくつか種類があり、目的に応じて使い分けることも学びました。活性化関数については Qiita などで何度か見たことがあったので、理解は早かったです。

また後半では、Python における具体的な実装に触れました。ここでは、**ニューラルネットワークの各層の計算は行列の計算としてまとめて行える**ことが重要だったと感じます。実際に NumPy を使って行列の積を計算しました。読み進める際には、書く層の行列計算ばかりに集中せず、全体としてどういった計算を行うかに目を向けることが必要だったと感じます。

# 4 章 ニューラルネットワークの学習

# 5 章 誤差逆伝播法

# 6 章 学習に関するテクニック

# 7 章 畳み込みニューラルネットワーク

# 8 章 ディープラーニング
